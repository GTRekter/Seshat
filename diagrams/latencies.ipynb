{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è±Ô∏è Latency and Error Analysis Notebook\n",
    "\n",
    "This notebook analyzes **request latency** and **error latency** percentiles from Fortio JSON output files generated during HTTP load testing experiments.\n",
    "\n",
    "## üß™ Experiments Covered\n",
    "\n",
    "1. **01 - HTTP Max Throughput**\n",
    "    - Compares p50, p75, p90, and p99 latency and error latencies between Istio and Linkerd at max throughput.\n",
    "\n",
    "2. **02 - HTTP Constant Throughput**\n",
    "    - Runs at constant QPS values: 1, 1000, and 10000.\n",
    "    - Compares latency distributions per mesh across different traffic loads.\n",
    "\n",
    "3. **03 - HTTP Payload Variation**\n",
    "    - Uses fixed QPS (100) with varying payload sizes: 0, 1000, and 10000 bytes.\n",
    "    - Explores how latency changes with request payload size.\n",
    "\n",
    "## üìÅ Expected Input\n",
    "\n",
    "- JSON files under `../results/<experiment>/latencies_<mesh>_<qps>_<payload>_<timestamp>.json`\n",
    "- Structure:\n",
    "    - Fortio's `DurationHistogram` and `ErrorsDurationHistogram` blocks\n",
    "    - Extracted percentiles: **p50, p75, p90, p99**\n",
    "\n",
    "## üìâ Output\n",
    "\n",
    "- Line plots for latency and error latency percentiles (per experiment config).\n",
    "- Output PNGs are saved in:\n",
    "    - `../diagrams/<experiment>/experiment*_latency*.png`\n",
    "    - `../diagrams/<experiment>/experiment*_error_latency*.png`\n",
    "\n",
    "> üß™ **Benchmark Goal**: Compare service mesh behavior under varying load and payload profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_percentiles(data, error=False):\n",
    "    \"\"\"\n",
    "    Extract percentiles (p50, p75, p90, p99) from a Fortio JSON blob.\n",
    "    \"\"\"\n",
    "    if error:\n",
    "        perc_list = data.get(\"ErrorsDurationHistogram\", {}).get(\"Percentiles\", [])\n",
    "    else:\n",
    "        perc_list = data.get(\"DurationHistogram\", {}).get(\"Percentiles\", [])\n",
    "    result = {}\n",
    "    for entry in perc_list:\n",
    "        p = entry.get(\"Percentile\")\n",
    "        if p in [50, 75, 90, 99]:\n",
    "            result[p] = entry.get(\"Value\")\n",
    "    return result\n",
    "\n",
    "def extract_params_from_filename(file_path):\n",
    "    \"\"\"\n",
    "    Parses a filename of the form:\n",
    "       latencies_<mesh>_<qps>_<payload>_<timestamp>.json\n",
    "    and returns (mesh, qps, payload).\n",
    "    \"\"\"\n",
    "    base = os.path.basename(file_path)\n",
    "    base = base[len(\"latencies_\"):]  # Remove prefix.\n",
    "    base = base.replace(\".json\", \"\")\n",
    "    parts = base.split(\"_\")\n",
    "    mesh = parts[0]\n",
    "    qps = int(parts[1])\n",
    "    payload = int(parts[2])\n",
    "    return mesh, qps, payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for experiment 1.\n",
    "experiment_dir = os.path.join(\"..\", \"results\", \"01_http_max_throughput\")\n",
    "diagram_dir = os.path.join(\"..\", \"diagrams\", \"01_http_max_throughput\")\n",
    "os.makedirs(diagram_dir, exist_ok=True)\n",
    "\n",
    "# Locate all latency JSON files for experiment 1.\n",
    "latency_files = glob.glob(os.path.join(experiment_dir, \"latencies_*.json\"))\n",
    "\n",
    "# Dictionaries to store results per mesh.\n",
    "results_latency = {}  # For the normal latency percentiles.\n",
    "results_error = {}    # For error latency percentiles.\n",
    "\n",
    "for file in latency_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    mesh, qps, payload = extract_params_from_filename(file)\n",
    "    # (For experiment 1, qps and payload should be 0.)\n",
    "    results_latency[mesh] = extract_percentiles(data, error=False)\n",
    "    results_error[mesh]   = extract_percentiles(data, error=True)\n",
    "\n",
    "# Define x-axis labels.\n",
    "x_labels = [\"p50\", \"p75\", \"p90\", \"p99\"]\n",
    "\n",
    "# Plot latency percentiles.\n",
    "plt.figure(figsize=(15, 10))\n",
    "for mesh, percs in results_latency.items():\n",
    "    # Order values for 50, 75, 90, and 99\n",
    "    y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "    plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Latency (seconds)\")\n",
    "plt.title(\"Experiment 1: Latency Percentiles\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "latency_output_path = os.path.join(diagram_dir, \"latency_0.png\")\n",
    "plt.savefig(latency_output_path)\n",
    "plt.close()\n",
    "\n",
    "# Plot error latency percentiles.\n",
    "plt.figure(figsize=(15, 10))\n",
    "for mesh, percs in results_error.items():\n",
    "    y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "    plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.ylabel(\"Error Latency (seconds)\")\n",
    "plt.title(\"Experiment 1: Error Latency Percentiles\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "error_output_path = os.path.join(diagram_dir, \"latency_error_0.png\")\n",
    "plt.savefig(error_output_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m latency_files:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     mesh, qps, payload = extract_params_from_filename(file)\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# (For experiment 2, payload is expected to be 0.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:293\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(fp, *, \u001b[38;5;28mcls\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, object_hook=\u001b[38;5;28;01mNone\u001b[39;00m, parse_float=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m         parse_int=\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant=\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook=\u001b[38;5;28;01mNone\u001b[39;00m, **kw):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[32m    278\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    341\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONDecoder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w=WHITESPACE.match):\n\u001b[32m    334\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03m    containing a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     end = _w(s, end).end()\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m end != \u001b[38;5;28mlen\u001b[39m(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.8/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m     obj, end = \u001b[38;5;28mself\u001b[39m.scan_once(s, idx)\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# %% [code] Experiment 2: Latency and Error Percentiles for Throughputs 1, 1000, and 10000\n",
    "\n",
    "# Define directories for experiment 2.\n",
    "experiment_dir = os.path.join(\"..\", \"results\", \"02_http_constant_throughput\")\n",
    "diagram_dir = os.path.join(\"..\", \"diagrams\", \"02_http_constant_throughput\")\n",
    "os.makedirs(diagram_dir, exist_ok=True)\n",
    "\n",
    "# Locate all latency JSON files for experiment 2.\n",
    "latency_files = glob.glob(os.path.join(experiment_dir, \"latencies_*.json\"))\n",
    "\n",
    "# Group results by QPS.\n",
    "results_latency = {}  # structure: { qps: {mesh: {50:..., 75:..., ... } } }\n",
    "results_error = {}    # similar structure for error histogram.\n",
    "\n",
    "for file in latency_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    mesh, qps, payload = extract_params_from_filename(file)\n",
    "    # (For experiment 2, payload is expected to be 0.)\n",
    "    if qps not in results_latency:\n",
    "        results_latency[qps] = {}\n",
    "        results_error[qps] = {}\n",
    "    results_latency[qps][mesh] = extract_percentiles(data, error=False)\n",
    "    results_error[qps][mesh]   = extract_percentiles(data, error=True)\n",
    "\n",
    "x_labels = [\"p50\", \"p75\", \"p90\", \"p99\"]\n",
    "\n",
    "# For each QPS value (1, 1000, 10000), generate latency and error plots.\n",
    "for qps_val in sorted(results_latency.keys()):\n",
    "    # Latency Plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for mesh, percs in results_latency[qps_val].items():\n",
    "        y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "        plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Latency (seconds)\")\n",
    "    plt.title(f\"Experiment 2: Latency Percentiles at QPS = {qps_val}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    latency_outfile = os.path.join(diagram_dir, f\"latency_{qps_val}.png\")\n",
    "    plt.savefig(latency_outfile)\n",
    "    plt.close()\n",
    "    print(\"Saved Experiment 2 latency diagram for QPS\", qps_val, \"to\", latency_outfile)\n",
    "    \n",
    "    # Error Latency Plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for mesh, percs in results_error[qps_val].items():\n",
    "        y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "        plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Error Latency (seconds)\")\n",
    "    plt.title(f\"Experiment 2: Error Latency Percentiles at QPS = {qps_val}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    error_outfile = os.path.join(diagram_dir, f\"latency_error_{qps_val}.png\")\n",
    "    plt.savefig(error_outfile)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for experiment 3.\n",
    "experiment_dir = os.path.join(\"..\", \"results\", \"03_http_payload\")\n",
    "diagram_dir = os.path.join(\"..\", \"diagrams\", \"03_http_payload\")\n",
    "os.makedirs(diagram_dir, exist_ok=True)\n",
    "\n",
    "# Locate all latency JSON files for experiment 3.\n",
    "latency_files = glob.glob(os.path.join(experiment_dir, \"latencies_*.json\"))\n",
    "\n",
    "# Group results by payload (qps is expected to be 100 in this experiment).\n",
    "results_latency = {}  # structure: { payload: {mesh: {50:..., 75:..., ... } } }\n",
    "results_error = {}    # similar grouping for error histograms.\n",
    "\n",
    "for file in latency_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    mesh, qps, payload = extract_params_from_filename(file)\n",
    "    if payload not in results_latency:\n",
    "        results_latency[payload] = {}\n",
    "        results_error[payload] = {}\n",
    "    results_latency[payload][mesh] = extract_percentiles(data, error=False)\n",
    "    results_error[payload][mesh]   = extract_percentiles(data, error=True)\n",
    "\n",
    "x_labels = [\"p50\", \"p75\", \"p90\", \"p99\"]\n",
    "\n",
    "# For each payload value, generate latency and error plots.\n",
    "for payload_val in sorted(results_latency.keys()):\n",
    "    # Latency Plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for mesh, percs in results_latency[payload_val].items():\n",
    "        y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "        plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Latency (seconds)\")\n",
    "    plt.title(f\"Experiment 3: Latency Percentiles (Throughput 100, Payload = {payload_val})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    latency_outfile = os.path.join(diagram_dir, f\"latency_100_{payload_val}.png\")\n",
    "    plt.savefig(latency_outfile)\n",
    "    plt.close()\n",
    "    \n",
    "    # Error Latency Plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for mesh, percs in results_error[payload_val].items():\n",
    "        y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "        plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Error Latency (seconds)\")\n",
    "    plt.title(f\"Experiment 3: Error Latency Percentiles (Throughput 100, Payload = {payload_val})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    error_outfile = os.path.join(diagram_dir, f\"latency_error_100_{payload_val}.png\")\n",
    "    plt.savefig(error_outfile)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for experiment 4.\n",
    "grpc_experiment_dir = os.path.join(\"..\", \"results\", \"04_grpc_max_throughput\")\n",
    "grpc_diagram_dir = os.path.join(\"..\", \"diagrams\", \"04_grpc_max_throughput\")\n",
    "os.makedirs(grpc_diagram_dir, exist_ok=True)\n",
    "\n",
    "# Locate all latency JSON files for experiment 4.\n",
    "grpc_latency_files = glob.glob(os.path.join(grpc_experiment_dir, \"latencies_*.json\"))\n",
    "\n",
    "# Group results by payload (or any parameter extracted from the filename).\n",
    "# The structure is: { payload: {mesh: {50: value, 75: value, 90: value, 99: value} } }\n",
    "results_latency_grpc = {}\n",
    "results_error_grpc = {}\n",
    "\n",
    "for file in grpc_latency_files:\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # This function is assumed to extract the mesh name, qps, and payload information from the filename.\n",
    "    mesh, qps, payload = extract_params_from_filename(file)\n",
    "    if payload not in results_latency_grpc:\n",
    "        results_latency_grpc[payload] = {}\n",
    "        results_error_grpc[payload] = {}\n",
    "    # extract_percentiles should parse the JSON and return a dictionary mapping percentile to value.\n",
    "    results_latency_grpc[payload][mesh] = extract_percentiles(data, error=False)\n",
    "    results_error_grpc[payload][mesh] = extract_percentiles(data, error=True)\n",
    "\n",
    "x_labels = [\"p50\", \"p75\", \"p90\", \"p99\"]\n",
    "\n",
    "# For each payload value, generate latency and error plots for GRPC.\n",
    "for payload_val in sorted(results_latency_grpc.keys()):\n",
    "    # ---------------------\n",
    "    # GRPC Latency Plot\n",
    "    # ---------------------\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for mesh, percs in results_latency_grpc[payload_val].items():\n",
    "        # The keys 50, 75, 90, and 99 should be available in the dictionary returned by extract_percentiles.\n",
    "        y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "        plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Latency (seconds)\")\n",
    "    plt.title(f\"GRPC: Latency Percentiles (Throughput 0, Payload = {payload_val})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    latency_outfile = os.path.join(grpc_diagram_dir, f\"latency_0_{payload_val}.png\")\n",
    "    plt.savefig(latency_outfile)\n",
    "    plt.close()\n",
    "    \n",
    "    # ------------------------------\n",
    "    # GRPC Error Latency Plot\n",
    "    # ------------------------------\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for mesh, percs in results_error_grpc[payload_val].items():\n",
    "        y_values = [percs.get(50), percs.get(75), percs.get(90), percs.get(99)]\n",
    "        plt.plot(x_labels, y_values, marker='o', label=mesh)\n",
    "    plt.xlabel(\"Percentile\")\n",
    "    plt.ylabel(\"Error Latency (seconds)\")\n",
    "    plt.title(f\"GRPC: Error Latency Percentiles (Throughput 0, Payload = {payload_val})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    error_outfile = os.path.join(grpc_diagram_dir, f\"latency_error_0_{payload_val}.png\")\n",
    "    plt.savefig(error_outfile)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
